<?xml version="1.0" encoding="us-ascii"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content=
  "HTML Tidy for Linux/x86 (vers 1 September 2005), see www.w3.org" />
  <meta http-equiv="Content-Type" content=
  "text/html; charset=us-ascii" />

  <title>SAFTest Next Generation - Requirements and High Level Design</title>
  <link type="text/css" rel="stylesheet" href="saftest_style.css" />
<script type="text/javascript" src="saftest_scripts.js">
</script>
</head>

<body onload="generateTOC(); numberSections();">
  <div id="header">
    <h1>SAFTest Next Generation - Requirements and High Level Design</h1>

    <p id="author">Chad Tindel &lt;chad@tindel.net&gt;</p>

    <p id="date">Last Updated 7 Feb 2006</p>

    <p id="description">Requirements and High Level Design of a Next 
    Generation SAFTest Framework.</p>

    <ul id="toc"></ul>
  </div>

  <div id="content">
    <div class="section">

      <h2>Overview</h2>

      <p>The <a href="http://sourceforge.net/projects/saftest">SAFTest 
      project</a> is an open source project which aims to create a common 
      criteria test suite that can be used for SAF Compliance testing.  The 
      idea is that this suite can be executed against any vendor's SAF 
      implementation, either by an official SAF compliance lab or by the vendor
      themselves, to guarantee that it meets a minimum level of library 
      compliance.  Of course it makes no distinction between the quality of one
      HA middleware product and another, so it is obviously still "<i>buyer 
      beware</i>" when it comes to purchasing any given SAF implementation.</p>

     <p>The current SAFTest system is focused mainly on compliance of calling
     conventions.  It exercises each function in the specification and ensures
     that the expected result is returned.  It generally tries to exercise valid
     and invalid boundary conditions in an attempt to see each function return 
     each possible return value.  Of course this isn't completely possible 
     (how can you cause an implementation dependent timeout, anyway?), but it 
     does the best that it can.</p>

     <p>The Next Generation design attempts to go beyond just mere API 
     compliance, and into the realm of functional testing.  For example, 
     because the current system has no way of causing a cluster reformation, 
     it can't actually exercise the SA_TRACK_CHANGES functionality of the CLM 
     specification.  There are many other examples as well, and these will be 
     covered in the rest of the document.</p>

     <p>In the current system, SAF implementors have to develop their own
     proprietary test suites to do functional testing.  For example, if an
     implementor wants to compare the data coming out of his SAF library against
     the data being returned by some implementation-specific command that is 
     already tested and known to work properly, the saftest project does not 
     help them with this task.  The Next Generation SAFTest project aims to not
     just be useful for SAF compliance testing, but to also be useful for 
     implementation developers.  In an ideal world, SAF implementation 
     developers would not need their own proprietary test-suite, but would 
     instead simply write their implementation specific plugins for the shared 
     SAFTest project.</p>

     <p>Be aware that this is a living document, and the proof-of-concept
     implementation is forever changing.</p>
    </div>

    <div class="section">

      <h2>The Current SAFTest System</h2>

      <p>As stated previously, the current SAFTest system focuses mainly on the
      issues of API compliance (does the implementation actually provide all 
      of the specification's function calls) and general conformance (does the 
      implemenation properly accept valid input and reject invalid input).</p>

      <p>The test cases are organized by directories of API function names, and
      each test case attempts to exercise attempts to exercise a specific 
      different piece of that function's specification.  For example, in the 
      CLM test suite there are directores for saClmInitialize, saClmFinalize, 
      etc.  Within the saClmInitialize directory there is one test case to call
      saClmInitialize with a valid SAF Version object, one to call 
      saClmInitialize with an invalid SAF Version object (i.e. a version that 
      doesn't exist yet like "C.01.01", one to call saClmInitialize with a 
      NULL SAF Version pointer, etc.  Each test case is just a regular C 
      program, and a simple test harness is provided to execute a batch of 
      test cases and to collect the results.</p>

      <p>Since each test case is meant to be fully complete, these cases tend to
      have an extreme amount of code duplication.  For example, the current CLM
      test suite has more 7000 lines of code (not counting comments).</p>

      <div class="section">
        <h3>Current SAFTest Limitations</h3>

        <p>The current system has several limitations (which will be addressed 
        by the new system design).  These are:</p>

        <ol type="1">
          <li><b>Inability to drive multiple clients simultaneously</b> - It
          is often useful to have multiple clients for each library, with the 
          test case controlling exactly when each client calls SAF functions.  
          For example, when testing the LCK library, you might want to have one
          client acquire a lock, then have a second client attempt to acquire 
          the lock.  This allows you to validate that the first client received
          a lock-waiter notification.  Then the test case can tell the first 
          client to unlock or it can just outright kill the first client, and 
          validate the lock is successfully acquired by the second client.</li>

          <li><b>Inability to validate that API data is correct</b> - The 
          current CLM test cases can call saClmClusterNodeGet(), but they have 
          no way of telling whether the node information returned by the API is
          correct.  This is typically handled by a manual test case, where the 
          case displays API information and asks a human if the data is 
          correct.</li>

          <li><b>Inability to cause cluster events to occur</b> - The current 
          CLM test cases might be able to register for a callback when a cluster
          reformation occurs, but since there is no way to cause a cluster 
          reformation there is no way to automatically exercise this 
          functionality.  This is typically handled by a manual test cases, 
          where the case does some setup, then asks a human to cause a cluster 
          reformation.</li>

          <li><b>Inability to exercise multiple SAF APIs from the same process 
          space</b> - The current test cases are standalone programs which 
          exercise a set of functions only from one SAF API.  Given this 
          architecture, it would be combinatorically expensive to write all 
          the different programs that would let vendors test their various SAF 
          APIs from within one process space.  However, this would be a very 
          useful thing to test because it more closely mimics what happens in 
          the real world.</li>
        </ol>
      </div>
    </div>

    <div class="section">

      <h2>Next Generation SAFTest</h2>
      <div class="section">
        <h3>Next Generation SAFTest Requirements</h3>

        <p>At a high level, the Next Generation SAFTest project attemps to 
        provide all the functionality of the old system, while also solving 
        all the problems of the old system.  This can be distilled to the 
        following goals:</p>

        <ol type="1">
          <li><b>Provide a good infrastructure for developing functional and 
          performance test cases</b> - SAF implementors should not need to 
          develop proprietary test suites just to validate that their 
          implementation works properly.  Since all implementors are sharing a 
          standard, in the spirit of openness they should also share a test 
          suite.  This will allow for much more rapid development of both the 
          implementation as well as prototypes for future SAF standards.</li>
          <li><b>No Manual Test Cases</b> - Programmers following the XP 
          development model write their test cases first, and are constantly 
          re-running their test suite as they make changes to their 
          implementation.  Somebody who might run their test suite 20 times a 
          day does not want to see manual test cases in their test suite.</li>
          <li><b>User-Friendly Test Execution</b> - It should be simple for a 
          test-executor to setup this test suite to execute against ANY 
          implementation, provided the implementation has included their 
          hook-point scripts in the SAFTest project.  You shouldn't need to 
          understand makefiles, compiler options, or any of that stuff to 
          setup and run the SAFTest Suite.</li>
        </ol>
      </div>

      <div class="section">
        <h3>Next Generation SAFTest High Level Design</h3>

        <p>The Next Generation SAFTest project has a completely different 
        design.  Instead of one C program for each test case, there is one 
        C-program per each SAF specification, called a <b>driver</b>.  This 
        driver has two pieces, a client piece and a daemon piece.  The daemon 
        is a long-lived process which accepts requests from the client and 
        exercises the actual SAF functions of the implementation.  The client 
        is a short-lived process, which typically does nothing more than ask 
        the daemon to exercise one SAF function and then handles the reply from
        the daemon.</p>

        <p>The test cases themselves are nothing more than scripts which start 
        a driver daemon instance (or multiple daemon instances) and drive them 
        through SAF function calls via the driver client.  These test cases are
        currently implmented in Ruby, which works very well for parsing output 
        and writing test cases quickly.  The really nice thing about using a 
        higher-level scripting language like Ruby is that there are already 
        frameworks for writing and executing test case suites, which is nice 
        because it means that we don't have to focus on infrastructure for 
        running test cases; all we have to focus on is writing the test cases 
        themselves.</p>

        <p>The <i>saf_driver</i> daemon is really kind of a shell which manages
        the incoming connections and client requests, but has no brains about 
        running any specific SAF functions.  For each SAF API there will be a 
        shared library which is responsible for calling the SAF functions that 
        are requested by the test clients.  For example, there will be a 
        <i>clm_driver.so</i> shared library which is where the calls to 
        functions such as <i>SaClmInitialize()</i> exist, and so on for 
        <i>lck_driver.so</i>, <i>msg_driver.so</i>, etc.  The <i>saf_driver 
        daemon</i> level is primarily responsible for accepting requests from 
        the clients, routing them to the appropriate shared library, and 
        delivering whatever reply message was generated by the shared library.
        The daemon instances can be long-lived and therefore can persists 
        across multiple test case instances or even for the entire duration of 
        the test run.  The picture below represents the SAFTest Next 
        Generation software stack:</p>

        <img src="images/saftest_case_communcation.png"/>

        <p>As you can see, this architecture makes it extremely easy to drive
        multiple SAF clients and SAF APIs from within one test case.  Simply 
        start multiple daemon instances (with each daemon linking in multiple 
        saf_driver libraries), and you have complete control over when each 
        daemon calls each SAF function.</p>

        <div class="section">
          <h4>Per-specification driver</h4>

          <p>Each "per-specification driver" (like clm_driver.so) will maintain
          a list of <b>test resources</b>.  A test resource is simply an
          abstraction on that specification's Handle type (like SaClmHandleT).
          Each test resource object represents one Handle.  When a test case
          wants to invoke a SAF function, it tells the driver which resource
          to use and which function to invoke, along with any other parameters
          that might be required by that function.  When the driver receives
          the request, it will lookup that test resource and use the
          corresponding Handle when invoking the SAF function.  A graphical
          representation of this is shown below:</p>

          <img src="images/saftest_driver_resources.png"/>
        </div>
      </div>

      <div class="section">
        <h3>Implementation Hooks</h3>

        <p>In order to provide a good architecture for automated (i.e. 
        non-manual) functional tests, the new design has to incorporate the 
        ability to gather data in an implementation-specific way.  There 
        simply is no way for the test harness to know how to cause a cluster 
        reformation (either by starting/stopping a node, or killing a node, 
        etc...), or how to gather the real cluster membership information.  
        Presumably HA-middleware vendors already have commands or APIs to 
        performing these functions or gather information without going through 
        the SAF libraries that are known to work properly, so the test 
        infrastructure just needs to provide hook-points for the 
        implementation to plugin their way of doing this.  For the 
        proof-of-concept there are the current hook-points: </p>

        <ol type="1">
          <li><b>Display Cluster Information</b> - Will generate XML output 
          that describes the same kind of information one would find in a
          SaClmClusterNotificationBufferT object.  Takes no command line 
          arguments.</li>

          <li><b>Start a node</b> - Will cause a node to join the cluster.  
          Takes only one command line argument, the node name.</li>

          <li><b>Stop a node</b> - Will cause a node to leave the cluster.  
          Takes only one command line argument, the node name.</li>

          <li><b>Add a node</b> - Will add a new node to the cluster
          configuration.  Takes only one command line argument, the node 
          name.</li>

          <li><b>Delete a node</b> - Will delete a node from the cluster
          configuration.  Takes only one command line argument, the node 
          name.</li>
        </ol>

        <p>Examples of the XML output are kept in the xml/examples directory.  
        At some point we should write schemas for each different kind of XML 
        document that we have so that the test implementation can validate 
        that appropriate output is being generated.  One example is shown 
        later in this section.</p>

        <p>Each implementation needs to provide a config file that points to the
        implementation-specific command for each hook-point.  The Serviceguard 
        example looks like this:</p>

        <pre class="example">
&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;SAFImplementation xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:noNamespaceSchemaLocation="SAFImplementation.xsd" schemaVersion="1"&gt;
    &lt;DisplayClusterCommand&gt;/root/src/saftest_ng/trunk/implementation/serviceguard/sg_cluster_display.rb&lt;/DisplayClusterCommand&gt;
    &lt;StartNodeCommand&gt;/usr/local/cmcluster/bin/cmrunnode&lt;/StartNodeCommand&gt;
    &lt;StopNodeCommand&gt;/usr/local/cmcluster/bin/cmhaltnode -f&lt;/StopNodeCommand&gt;
    &lt;AddNodeCommand&gt;/root/src/saftest_ng/trunk/implementation/serviceguard/sg_node_add.rb&lt;/AddNodeCommand&gt;
    &lt;DeleteNodeCommand&gt;/root/src/saftest_ng/trunk/implementation/serviceguard/sg_node_delete.rb&lt;/DeleteNodeCommand&gt;
&lt;/SAFImplementation&gt;</pre>

        <p>So, when a test case needs to gather the current cluster 
        information, it will call the script specified by DISPLAY_CLUSTER, and 
        harvest the relevant information from the XML output.  An example of 
        this kind of output is:</p>

<pre class="example">
&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;SAFCluster  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:noNamespaceSchemaLocation="SAFCluster.xsd"  schemaVersion="1"&gt;
    &lt;SAFNodeList&gt;
        &lt;SAFNode&gt;
            &lt;id&gt;1&lt;/id&gt;
            &lt;AddressList&gt;
                &lt;Address&gt;
                    &lt;family&gt;SA_CLM_AF_INET&lt;/family&gt;
                    &lt;length&gt;4&lt;/length&gt;
                    &lt;value&gt;192.168.73.10&lt;/value&gt;
                &lt;/Address&gt;
            &lt;/AddressList&gt;
            &lt;name&gt;sg1&lt;/name&gt;
            &lt;member&gt;TRUE&lt;/member&gt;
            &lt;bootTimestamp&gt;0&lt;/bootTimestamp&gt;
            &lt;initialViewNumber&gt;0&lt;/initialViewNumber&gt;
        &lt;/SAFNode&gt;
        &lt;SAFNode&gt;
            &lt;id&gt;2&lt;/id&gt;
            &lt;AddressList&gt;
                &lt;Address&gt;
                    &lt;family&gt;SA_CLM_AF_INET&lt;/family&gt;
                    &lt;length&gt;4&lt;/length&gt;
                    &lt;value&gt;192.168.73.20&lt;/value&gt;
                &lt;/Address&gt;
            &lt;/AddressList&gt;
            &lt;name&gt;sg2&lt;/name&gt;
            &lt;member&gt;TRUE&lt;/member&gt;
            &lt;bootTimestamp&gt;0&lt;/bootTimestamp&gt;
            &lt;initialViewNumber&gt;0&lt;/initialViewNumber&gt;
        &lt;/SAFNode&gt;
    &lt;/SAFNodeList&gt;
&lt;/SAFCluster&gt;
</pre>

        <p>Similarly, when a testcase needs to halt a node it will append the 
        node name to the STOP_NODE command and run that.  For example:</p>

        <p># <i>cmhaltnode -f sg1</i></p>

        <p>I imagine that as people want to develop more and more complicated 
        functional tests, more hook-points will be useful, and adding them is 
        trivial.  These help us meet the goal of validating SAF output without 
        manual test cases as well as causing cluster events in an 
        implementation specific-fashion.</p>

      </div>
    </div>

    <div class="section">
      <h2>Open Issues</h2>

      <ul>
        <li>Define XML schemas for each XML document</li>
        <li>Create a configuration utility that allows the user to tell saftest
        which libraries need to be tested.</li>
        <li>Automatically create the result directories</li>
        <li>Convert the implementation conf file into XML format</li>
        <li>Abstract each specification's Make rules into a 
        specification-specific include file</li>
        <li>I assume we'll need an implementation specific LDFLAGS variable</li>
        <li>Figure out what to do about test report generation</li>
        <li>Add hooks for configuring and starting a cluster</li>
      </ul>

    </div>
  </div>
</body>
</html>
